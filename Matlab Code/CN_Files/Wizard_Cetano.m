% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created 04-Jun-2020 17:42:01
%
% This script assumes these variables are defined:
%
%   Input - input data.
%   Target - target data.

x = Input';
t = Target';

% Choose a Training Function
% For a list of all training funtions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.
trainFcn = 'trainbr';  % Bayesian Regularization backpropagation.

% Create a Fitting Network

for h = 10
    m=1; %iterador que sirve para guardar información
    for nrow = 1:10
        hiddenLayerSize = [h];   
        net = fitnet(hiddenLayerSize,trainFcn);
        display(nrow)
        display(h)
        
        % Choose Input and Output Pre/Post-Processing Functions
        % For a list of all processing functions type: help nnprocess
        net.input.processFcns = {'removeconstantrows','mapminmax'};
        net.output.processFcns = {'removeconstantrows','mapminmax'};

        % Setup Division of Data for Training, Validation, Testing
        % For a list of all data division functions type: help nndivision
        net.divideFcn = 'divideind';  % Divide data randomly
        net.divideMode = 'sample';  % Divide up every sample
        net.divideParam.testInd = ind_iniTest:ind_finTest;
        net.divideParam.trainInd = ind_iniTrain:ind_finTrain;
        net.divideParam.valInd = ind_iniVal:ind_finVal;

        % Choose a Performance Function
        % For a list of all performance functions type: help nnperformance
        net.performFcn = 'mse';  % Mean Squared Error

        % Choose Plot Functions
        % For a list of all plot functions type: help nnplot
        net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
            'plotregression', 'plotfit','plotShallowNetwork'};

        % Train the Network
        [net,tr] = train(net,x,t);
        net.trainParam.mu = 3e-6;       
        net.trainParam.show = 25;
        net.trainParam.mu_dec = 0.01;
       
        % Test the Network
        y = net(x);
        e = gsubtract(t,y);
        performance = perform(net,t,y);

        % Recalculate Training, Validation and Test Performance
        trainTargets = t .* tr.trainMask{1};
        valTargets = t .* tr.valMask{1};
        testTargets = t .* tr.testMask{1};
        trainPerformance = perform(net,trainTargets,y);
        valPerformance = perform(net,valTargets,y);
        testPerformance = perform(net,testTargets,y);
        rmsePerformance = sqrt(performance);
        rmsetrainPerformance = sqrt(trainPerformance);
        rmsetestPerformance = sqrt(testPerformance);
        
        % View the Network
        %view(net)

        % Plots
        % Uncomment these lines to enable various plots.
        %figure, plotperform(tr)
        %figure, plottrainstate(tr)
        %figure, ploterrhist(e)
        %figure, plotregression(t,y)
        %figure, plotfit(net,x,t)

        % Deployment
        % Change the (false) values to (true) to enable the following code blocks.
        % See the help for each generation function for more information.
        if (false)
            % Generate MATLAB function for neural network for application
            % deployment in MATLAB scripts or with MATLAB Compiler and Builder
            % tools, or simply to examine the calculations your trained neural
            % network performs.
            genFunction(net,'myNeuralNetworkFunction');
            y = myNeuralNetworkFunction(x);
        end
        if (true)
            % Generate a matrix-only MATLAB function for neural network code
            % generation with MATLAB Coder tools.
            genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
            y = myNeuralNetworkFunction(x);
        end
        if (false)
            % Generate a Simulink diagram for simulation or deployment with.
            % Simulink Coder tools.
            gensim(net);
        end
        
        % Número de los datos
         n = length(Input);
        
        % Predicciones de la red neuronal Predicted
        Predict = myNeuralNetworkFunction(x);
        residuales = abs(t-Predict);
        
        % Validación de los restulados MAE       
        MAE = sum(residuales')/n;
        
        % Relevancia de los descriptores Alpha
        j=m:m+1 %ITERADOR ESTE PASO EN EL CODIGO ES IMPORTANTE
        alpha_data = weigth_alpha(net,selected_input);
        resultados_alpha{1,h}.alpha(:,j) = alpha_data;
        m=m+2; %ITERADOR ESTE PASO EN EL CODIGO ES IMPORTANTE
        
        % Correlación entre la predicción y target R
        rtest=corrcoef(t(ind_iniTest:ind_finTest),Predict(ind_iniTest:ind_finTest));
        rtrain=corrcoef(t(ind_iniTrain:ind_finTrain),Predict(ind_iniTrain:ind_finTrain));
        rval=corrcoef(t(ind_iniVal:ind_finVal),Predict(ind_iniVal:ind_finVal));
        rall=corrcoef(t,Predict);
        
        rtest = rtest(1,2); rtrain = rtrain(1,2); rall = rall(1,2); 
        rval = rval(1,2);
        
        % RMSE validation
        residual_val = abs(t(ind_iniVal:ind_finVal)-Predict(ind_iniVal:ind_finVal));
        MAE_val = sum(residual_val')/length(residual_val);
        rmseValidation = sqrt(MAE_val);
        
        % Matriz de resultados RMSE,MAE,R 
        resultados_prediccion{1,h}.mse(nrow,:) = Predict;
        
        resultados_performance{1,h}.mse(nrow,:) = table(rtrain, rtest, rval,...
        rall,rmsePerformance,rmsetrainPerformance,rmsetestPerformance,...
        rmseValidation, MAE,h);
    
        % Dir Folder Save
        dir = 'C:\Users\Cristopher\Documents\AVANCES-TESIS\Avance_2\CN_Test\Red2-Louvain';
        
        % Guardar funciones
        renameFunction = strcat('funcionHidden_',int2str(h),'_',int2str(nrow),'.m');
        movefile('myNeuralNetworkFunction.m',renameFunction,'f');
        movefile(renameFunction,dir);
        
        % Guardar net
        save net
        renameNet = strcat('net_',int2str(h),'_',int2str(nrow),'.mat');
        movefile('net.mat',renameNet,'f');
        movefile(renameNet,dir);
        
        % Guardar MSE
        renameMSE = strcat('MSE_',int2str(h),'_',int2str(nrow));
        saveas(plotperform(tr),renameMSE)
        movefile(strcat(renameMSE,'.fig'),dir);
        
        % Guardar Hist
        renameHist = strcat('Hist_',int2str(h),'_',int2str(nrow));
        saveas(ploterrhist(e),renameHist)
        movefile(strcat(renameHist,'.fig'),dir);
       
        % Guardar R
        renameR = strcat('R_',int2str(h),'_',int2str(nrow));
        saveas(plotregression(t,y), renameR)
        movefile(strcat(renameR,'.fig'),dir);


    end
end


exportar_datos = [];
for m = h % h es el rango de los datos 
    mse = resultados_performance{1,m}.mse(:,:);
    exportar_datos = [exportar_datos; (mse)];
end

%%

%Weights matrix data output, and relative relevance of FG
%weights = getwb(net);
wb = formwb(net,net.b,net.iw,net.lw);
[b,iw,lw] = separatewb(net,wb);
IW_1 = iw{:,:};
[rown, coln] = size(IW_1);
suma_Wij2 = 1./sum(IW_1.^2);
alpha = log10(suma_Wij2);
%min_alpha = min(alpha);
%alpha_plus = normalize(alpha');
alpha_plus = abs(alpha);

%Labels data 
char = cellstr(selected_input(:));

%Sort the data & Rearrenge Labels
[sorted_data, new_indices] = sort(alpha_plus,'descend'); %sort in des order
sorted_labels = char(new_indices);

%Plot in Descending Order
figure();
b=bar(sorted_data', 0.6);
b.FaceColor = [0 0.5 0.5];
set(gca, 'xticklabel', sorted_labels)
xticks(1:coln)
xtickangle(45)
title('Relevancia')
grid on

%%
%Error data target-predicted
Predict = myNeuralNetworkFunction(x)';
Error = t'-Predict;
Resultados = table(t',Predict,Error);

%%
matrixCorrelacion = corr(Input); %matriz de correlación
pValue =  fitlm(Input,Target); %pValue p>0.05 candidato a eliminar
